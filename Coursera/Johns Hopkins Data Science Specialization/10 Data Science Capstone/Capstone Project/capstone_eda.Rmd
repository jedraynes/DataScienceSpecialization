---
title: "Data Science Capstone Week 2 Milestone Report"
author: "jedraynes"
date: "4/6/2021"
output: html_document
---


```{r setup, message = FALSE, results = 'hide'}
# setup
knitr::opts_knit$set(root.dir = 'C:\\Users\\Jed\\iCloudDrive\\Documents\\Learn\\R\\Johns Hopkins Data Science Specialization\\10 Data Science Capstone\\Capstone Project\\final\\en_US')
```


```{r, message = FALSE, results = 'hide'}
# load libraries
suppressPackageStartupMessages(pacman::p_load(tidyverse, 
                                              stopwords, 
                                              wordcloud, 
                                              ggplot2, 
                                              tidytext, 
                                              reshape2, 
                                              tm))

# set seed
set.seed(1337)

# memory limit
memory.limit(size = 56000)

# prevent scientific notation
options(scipen=10000)
```


```{r}
# load in the data
twitter <- readLines('en_US.twitter.txt')
news <- readLines('en_US.news.txt')
blogs <- readLines('en_US.blogs.txt')

# line counts
paste('There are ', length(twitter), ' lines in the Twitter dataset.', sep = '')
paste('There are ', length(news), ' lines in the News dataset.', sep = '')
paste('There are ', length(blogs), ' lines in the Blogs dataset.', sep = '')
```

```{r}
# convert the data to a dataframe
twitter <- data.frame(txt = twitter, stringsAsFactors = FALSE)
news <- data.frame(txt = news, stringsAsFactors = FALSE)
blogs <- data.frame(txt = blogs, stringsAsFactors = FALSE)
```


```{r}
# filter out punctuation, non-ascii characters, and numbers
twitter <- twitter %>%
  mutate(txt = tolower(txt)) %>%
  mutate(txt = str_remove_all(txt, '[[:punct:]]')) %>%
  mutate(txt = str_remove_all(txt, '[^[:ascii:]]')) %>%
  mutate(txt = str_remove_all(txt, '[[:digit:]]'))

news <- news %>%
  mutate(txt = tolower(txt)) %>%
  mutate(txt = str_remove_all(txt, '[[:punct:]]')) %>%
  mutate(txt = str_remove_all(txt, '[^[:ascii:]]')) %>%
  mutate(txt = str_remove_all(txt, '[[:digit:]]'))

blogs <- blogs %>%
  mutate(txt = tolower(txt)) %>%
  mutate(txt = str_remove_all(txt, '[[:punct:]]')) %>%
  mutate(txt = str_remove_all(txt, '[^[:ascii:]]')) %>%
  mutate(txt = str_remove_all(txt, '[[:digit:]]'))
```


```{r}
# n-gram
# source: https://www.r-bloggers.com/2019/08/how-to-create-unigrams-bigrams-and-n-grams-of-app-reviews/

# merge dataframe
all_text <- rbind(twitter, news)
all_text <- rbind(all_text, blogs)

```

```{r}
# unigram with frequencies to show coverage
unigram_freq <- all_text %>%
  unnest_tokens(output = word, input = txt) %>%
  count(word, sort = TRUE) %>% 
  mutate(freq = n / sum(n)) %>%
  mutate(cumulative_sum = cumsum(freq))

# wordcloud
wordcloud(words = unigram_freq$word, freq = unigram_freq$n, min.freq = 2, 
          max.words = 50, random.order = FALSE, rot.per = 0.25,
          colors = rev(palette())); text(0.5, 1, 'Wordcloud: Top 50 Words Across All Data Sources', cex = 1.5); text(.95, 0, 'Source: Twitter, Blogs, and News Sources per JHU/SwiftKey', cex = 0.75)
```

```{r}
# get dataframes with n% frequency
coverage_50_percent <- unigram_freq %>%
  filter(cumulative_sum <= 0.5)

coverage_90_percent <- unigram_freq %>%
  filter(cumulative_sum <= 0.9)

# paste lengths to determine the count of words giving the frequency
paste('Number of words with 50% coverage: ',dim(coverage_50_percent)[1],'. ','Number of words with 90% coverage: ', dim(coverage_90_percent)[1],'.', sep = '')
```

```{r}
# unigram
all_text_unigram <- all_text %>%
  unnest_tokens(output = word, input = txt) %>%
  count(word, sort = TRUE) %>%
  drop_na()

all_text_unigram %>%
  head(25) %>%
  ggplot() +
  geom_bar(mapping = aes(x = reorder(word, n), y = n), stat = 'identity', fill = 'cadetblue') +
  labs(title = 'Top 25 Unigrams Across All Data Sources', 
       caption = 'Source: Twitter, Blogs, and News Sources per JHU/SwiftKey') +
  xlab('Word') + 
  ylab('Count') +
  coord_flip() +
  theme_classic() + 
  theme(plot.title = element_text(size = 12, face = 'bold'), 
        plot.caption = element_text(size = 8, face = 'italic'))
```



```{r}
# twitter

# bigram - twitter only
twitter_bigram <- twitter %>%
  unnest_tokens(output = word, input = txt, token = 'ngrams', n = 2) %>%
  count(word, sort = TRUE) %>%
  drop_na()

twitter_bigram %>%
  head(25) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = reorder(word, n), y = n), stat = 'identity', fill = 'cadetblue') +
  labs(title = 'Top 25 Bigrams from Twitter', 
       caption = 'Source: Twitter Source per JHU/SwiftKey') +
  xlab('Word') + 
  ylab('Count') +
  coord_flip() +
  theme_classic() + 
  theme(plot.title = element_text(size = 12, face = 'bold'), 
        plot.caption = element_text(size = 8, face = 'italic'))
```


```{r}
# news

# bigram - news only
news_bigram <- news %>%
  unnest_tokens(output = word, input = txt, token = 'ngrams', n = 2) %>%
  count(word, sort = TRUE) %>%
  drop_na()

news_bigram %>%
  head(25) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = reorder(word, n), y = n), stat = 'identity', fill = 'cadetblue') +
  labs(title = 'Top 25 Bigrams from News', 
       caption = 'Source: News Source per JHU/SwiftKey') +
  xlab('Word') + 
  ylab('Count') +
  coord_flip() +
  theme_classic() + 
  theme(plot.title = element_text(size = 12, face = 'bold'), 
        plot.caption = element_text(size = 8, face = 'italic'))
```


```{r}
# blogs

# bigram - blogs only
blogs_bigram <- blogs %>%
  unnest_tokens(output = word, input = txt, token = 'ngrams', n = 2) %>%
  count(word, sort = TRUE) %>%
  drop_na()

blogs_bigram %>%
  head(25) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = reorder(word, n), y = n), stat = 'identity', fill = 'cadetblue') +
  labs(title = 'Top 25 Bigrams from Blogs', 
       caption = 'Source: Blogs Source per JHU/SwiftKey') +
  xlab('Word') + 
  ylab('Count') +
  coord_flip() +
  theme_classic() + 
  theme(plot.title = element_text(size = 12, face = 'bold'), 
        plot.caption = element_text(size = 8, face = 'italic'))
```


```{r}
# all text bigram
all__text_bigram <- all_text %>%
  unnest_tokens(output = word, input = txt, token = 'ngrams', n = 2) %>%
  count(word, sort = TRUE) %>%
  drop_na()

# trigram
all__text_trigram <- all_text %>%
  unnest_tokens(output = word, input = txt, token = 'ngrams', n = 3) %>%
  count(word, sort = TRUE) %>%
  drop_na()

all__text_trigram %>%
  head(25) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = reorder(word, n), y = n), stat = 'identity', fill = 'cadetblue') +
  labs(title = 'Top 25 Bigrams from Blogs', 
       caption = 'Source: Blogs Source per JHU/SwiftKey') +
  xlab('Word') + 
  ylab('Count') +
  coord_flip() +
  theme_classic() + 
  theme(plot.title = element_text(size = 12, face = 'bold'), 
        plot.caption = element_text(size = 8, face = 'italic'))
```